\documentclass[a4paper,10pt]{article} % type, taille police

\usepackage[utf8]{inputenc} % encodage
\usepackage[T1]{fontenc} % encodage
\usepackage[french]{babel} % gestion du français
\usepackage{amssymb} % symboles mathématiques
\usepackage{textcomp} % flèche,  intervalle
\usepackage{stmaryrd} % intervalle entiers

\usepackage[left=3cm,right=3cm,top=3cm,bottom=3cm]{geometry} % marges
\usepackage[hidelinks]{hyperref} % sommaire interactif dans un pdf
\usepackage[nottoc, notlof, notlot]{tocbibind} % affichage des références dans la table des matières (?)
\usepackage{float} % placement des figures
\usepackage[toc,page]{appendix} % ajout d'annexes
\usepackage{amsthm} % format des déf, prop...
\usepackage{amsmath} % matrices, ...
\usepackage{multirow} % fusionner cellules verticalement

\usepackage{tikz} % affichage de schémas
\usepackage{graphicx} % affichage d'images
\usepackage{url} % inclure des urls
\usepackage{bbold} % fonction caractéristique 1

\renewcommand{\appendixtocname}{Annexes} % renommage annexes
\renewcommand{\appendixpagename}{Annexes}

\definecolor{bgreen}{rgb}{0.30,0.70,0}

\theoremstyle{definition} % pas d'italique pour le format des déf, prop...
\newtheorem{thm}{Théorème} % \begin{thm} \end{thm}
\newtheorem{cor}[thm]{Corollaire}
\newtheorem{defi}[thm]{Définition}
\newtheorem{ex}[thm]{Exemple}
\newtheorem{lem}[thm]{Lemme}
\newtheorem{rem}[thm]{Remarque}
\newtheorem{conj}[thm]{Conjecture}

\newcommand{\R}{\mathbb{R}}
\newcommand{\Q}{\mathbb{Q}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\Z}{\mathbb{Z}}

%#############################################################################################################%
%#############################################################################################################%
%#############################################################################################################%

\title{Reconnaissance et indexation de formes}
\author{Quentin Cormier \and Yassine Hamoudi}
\date{4 mai 2015}

\begin{document}

\maketitle

\tableofcontents

%#############################################################################################################%
%#############################################################################################################%
%#############################################################################################################%

\section{Introduction}

Il existe de nombreuses manières de caractériser une image en vue de reconnaître l'objet qu'elle représente. Cependant, la plupart des descripteurs ``élémentaires'' (périmètre de l'image, nombre de composantes connexes, surface ...) sont peu efficaces individuellement et nécessitent d'être combinés astucieusement pour donner de bons résultats.

Nous avons recherché un moyen robuste et plus concis de classifier en 70 catégories le millier d'images du projet. Notre méthode consiste à étudier la propagation d'une onde à l'intérieur d'une image. En effet, cette propagation dépend directement de la forme de l'objet. De la même manière que deux tambours de forme circulaire renvoient des sons proches, deux images de pommes devraient propager les ondes de façon similaires. Par ailleurs, cette méthode semble insensible à de nombreuses perturbations (rotation, translation, dilatation...).

Notre travaille s'inspire de \cite{Zuliani04} et \cite{KhabouHR07}. Nous étudierons dans un premier temps quels descripteurs peuvent être construits à partir du problème de la propagation d'une onde dans une image. Nous chercherons également la métrique la plus appropriée pour comparer deux vecteurs descripteurs associés à deux images. La robustesse de notre algorithme à différentes perturbations (rotation, bruit...) sera également vérifiée théoriquement et expérimentalement.

%#############################################################################################################%
%#############################################################################################################%
%#############################################################################################################%

\section{Méthode}

L'objectif est de classifier un certain nombre d'images noir et blancs en une catégorie parmi 70 possibles.
On dispose pour cela d'une dizaine d'images "d'entraînement" par catégories (le \textit{training set}, noté $T$).

A partir de ces images d'entrainement on doit pouvoir classifier une image inconnu dans l'une de ses 70 catégories.

Au vu de la taille relativement restreinte du \textit{training set}, on décide d'encoder l'image en un vecteur de réels (le \textit{descripteur} associé à l'image), et d'utiliser 
une distance entre deux descripteurs pour classifier une image inconnue.

Ainsi, pour une image inconnue $M$, on calcule le descripteur associé $d_M$, puis on cherche l'image la plus proche dans le \textit{training set} : $$M' = argmin_{M' \in T}{d(d_M, d_{M'})}_,   $$ $d$ étant la distance choisie.

On prédit alors la classe de $M$ comme étant la même que celle de $M'$


\subsection{Descripteur associé à l'image}

La partie principale du projet est donc le calcul du descripteur associé à l'image. Ce vecteur de réels, doit varier en fonction de la forme de l'image.
Idéalement, deux images ayant des formes similaires doivent donner deux descripteurs similaires.

Dans la mesure du possible, il doit être également invariant par translation, par rotation et par dilatation : translater une image, la tourner, l'agrandir ou la rétrécir ne change pas la catégorie de l'image, et donc ne devrait pas changer le descripteur.
On aimerait également une robustesse au bruit : un léger bruit ne doit que modifier très légèrement le descripteur.

On choisit ici d'utiliser un descripteur inspiré par la physique qui respecte, on le verra, toutes ces contraintes.

\subsection*{Onde associé à un domaine}

En physique, il est bien connu qu'une onde stationnaire se propageant dans une cavité ne peut se propager seulement qu'en certaines fréquences discrètes qui dépendent de la forme de la cavité.

Une onde dans une cavité vérifie l'équation suivante : $$\Delta E + \frac{1}{v^2} \frac{\partial^2 E}{\partial t^2} = 0$$ avec par exemple : $E$ nulle sur les bords de la cavité (condition aux limites de Dirichlet).

\subsection{Normalisation des images}

% mettre des exemples d'images après normalisation

%#############################################################################################################%
%#############################################################################################################%
%#############################################################################################################%

\section{Résultats}

Nous exposons les résultats obtenus grâce à la méthode détaillée précédemment. Nous allons étudier dans un premier temps la sensibilité de notre algorithme aux perturbations (rotation, redimensionnement, bruit, ...), puis nous détaillerons les résultats de classification sur le dataset d'images.

%-------------------------------------------------------------------------------------------------------------%

  \subsection{Sensibilité aux perturbations}

Les valeurs propres du Laplacien de Dirichlet vérifient un certain nombre de propriétés mathématiques qui garantissent que notre descripteur est insensible au redimensionnement, à la rotation et à la translation. Nous vérifions expérimentalement ces propriétés ci-dessous.

~\\
\underline{Redimensionnement} Etant donné un domaine $\Omega$ et un facteur $a > 0$, on a $\lambda_k(a \Omega) = \frac{\lambda_k(\Omega)}{a^2}$ (voir \cite{KhabouHR07}). Or, notre descripteur utilise des rapports de valeurs propres, il est donc inchangé par redimensionnement : $\frac{\lambda_k(a \Omega)}{\lambda_m(a \Omega)} = \frac{\lambda_k(\Omega)}{\lambda_m(\Omega)}$. Nous avons calculé différents rapports pour l'image \texttt{camel-1.pgm}, les résultats figurent table \ref{scale}. Les variations d'une image à l'autre peuvent s'expliquer par les dégrations des contours suite au redimensionnement. Les rapports restent tout de même très proches. En pratique, nous utilisons une longueur de 50 pixels afin d'obtenir des temps de calcul raisonnables (environ 20s pour calculer les valeurs propres associées à une image de taille 50x50).

\begin{table}[H]
  \begin{center}
    \begin{tabular}{l | c c c c c c}
                & $\lambda_1 / \lambda_2$ & $\lambda_1 / \lambda_3$ & $\lambda_1 / \lambda_4$ & $\lambda_2 / \lambda_3$ & $\lambda_3 / \lambda_4$ & $\lambda_4 / \lambda_5$ \\ \hline
      75 pixels & 0.63 & 0.42 & 0.36 & 0.67 & 0.84 & 0.89 \\
      50 pixels & 0.61 & 0.42 & 0.32 & 0.68 & 0.83 & 0.89 \\
      25 pixels & 0.53 & 0.39 & 0.31 & 0.73 & 0.80 & 0.81
    \end{tabular}
  \end{center}
\caption{Rapports de valeurs propres en fonctions de la longueur de l'image \texttt{camel-1.pgm} redimensionnée (l'image initiale est de taille 346x346)}
\label{scale}
\end{table}

~\\
\underline{Translation} Nous recadrons systématiquement l'image afin de conserver le plus petit rectangle contenant la figure. Ceci nous permet d'être insensible aux translations.

~\\
\underline{Rotation} Il a été démontré mathématiquement que les valeurs propres sont inchangées lorsque le domaine subit une rotation (voir \cite{Zuliani04}). Ce résultat se vérifie facilement à partir de quatre rotations appliquées sur \texttt{deer-20.pgm}. Les valeurs propres associées à chaque figure sont regroupées dans la table \ref{rot}.

\begin{center}
  \begin{tabular}{c c c c}
    \includegraphics[scale=0.15]{rotate/0.png} & \includegraphics[scale=0.15]{rotate/30.png} & \includegraphics[scale=0.15]{rotate/120.png} & \includegraphics[scale=0.15]{rotate/220.png} \\
    0 degré & 30 degrés & 120 degrés & 220 degrés
  \end{tabular}
\end{center}

\begin{table}[H]
  \begin{center}
    \begin{tabular}{c | c c c c c c c c}
                & $\lambda_1$ & $\lambda_2$ & $\lambda_3$ & $\lambda_4$ & $\lambda_5$ & $\lambda_6$ & $\lambda_7$ & $\lambda_8$ \\ \hline
      0 degré    & 118.57 & 177.16 & 252.35 & 285.53 & 361.22 & 404.89 & 420.69 & 489.36 \\
      30 degrés  & 120.28 & 182.75 & 257.54 & 297.97 & 370.39 & 413.53 & 426.99 & 472.08 \\
      120 degrés & 120.29 & 182.88 & 266.09 & 308.13 & 372.66 & 414.83 & 428.60 & 472.21 \\
      220 degrés & 120.42 & 182.95 & 250.94 & 299.81 & 366.01 & 406.55 & 424.32 & 478.39 
    \end{tabular}
  \end{center}
  \caption{Valeurs propres associées à \texttt{deer-20.pgm} en fonctions de l'angle de rotation}
  \label{rot}
\end{table}

~\\
\underline{Bruit} Le bruit peut déformer le domaine de l'image et modifier par conséquent les valeurs propres. Cependant, les premières valeurs propres ($\lambda_1, \lambda_2, \lambda_3, \dots$) correspondent à la fondamentale et aux premières harmoniques, et donc aux composantes de la solution au Laplacien de Dirichlet de longueur d'onde élevé. Par conséquent, on peut s'attendre à ce qu'une déformation relativement faible du contour impacte peu ces valeurs (contrairement aux valeurs associées à des longueurs d'onde faibles). {\large \textcolor{red}{Vrai ?}} Par ailleurs, le redimensionnement systématique de l'image que l'on applique permet de gommer partiellement le bruit.

Afin de tester la robustesse au bruit, nous avons implémenté un modèle de bruit de Kanungo (pour un facteur de bruit $0 \leq a \leq 1$, tout point $x$ du domaine à distance $d$ du bord est colorié en noir avec probabilité $a^d$). La table \ref{bruit} démontre le faible impact du bruit sur les valeurs propres.

\begin{center}
  \begin{tabular}{c c c c}
    \includegraphics[scale=0.15]{noise/0.png} & \includegraphics[scale=0.15]{noise/0_3.png} & \includegraphics[scale=0.15]{noise/0_6.png} & \includegraphics[scale=0.15]{noise/0_9.png} \\
    Bruit 0 & Bruit 0.3 & Bruit 0.6 & Bruit 0.9
  \end{tabular}
\end{center}

\begin{table}[H]
  \begin{center}
    \begin{tabular}{c | c c c c c c c c}
                 & $\lambda_1$ & $\lambda_2$ & $\lambda_3$ & $\lambda_4$ & $\lambda_5$ & $\lambda_6$ & $\lambda_7$ & $\lambda_8$ \\ \hline
      Bruit 0    & 83.93 & 153.41 & 258.76 & 263.10 & 353.64 & 378.19 & 495.73 & 500.11 \\
      Bruit 0.3  & 84.20 & 154.12 & 259.27 & 264.12 & 354.84 & 379.19 & 498.19 & 504.15 \\
      Bruit 0.6  & 84.42 & 155.22 & 262.05 & 264.96 & 355.90 & 388.19 & 503.35 & 514.37 \\
      Bruit 0.9  & 87.02 & 157.19 & 264.77 & 276.22 & 363.54 & 388.21 & 506.28 & 522.61  
    \end{tabular}
  \end{center}
  \caption{Valeurs propres associées à \texttt{beetle-13.pgm} en fonctions du facteur de bruit}
  \label{bruit}
\end{table}

%-------------------------------------------------------------------------------------------------------------%

  \subsection{Performances}

Nous avons testé notre algorithme avec 4 distances différentes: 
\begin{itemize}
  \item Distance euclidienne : $d(x,y) = \sqrt{\sum_{i=1}^n (x_i-y_i)^2}$.
  \item Distance de Minkowski (facteur 10) : $d(x,y) = \left(\sum_{i=1}^n (x_i-y_i)^{10}\right)^{1/10}$.
  \item Distance euclidienne carrée (sqeuclidean) : $d(x,y) = \sum_{i=1}^n (x_i-y_i)^2$.
  \item Distance cosine : $d(x,y) = 1 - \frac{\sum_{i=1}^n (x_i-y_i)^2}{\lVert x \rVert_2 \lVert y \rVert_2}$.
\end{itemize}

Nous avons utilisé soit des descripteurs de type 1 ($\lambda_1/\lambda_2, \lambda_1/\lambda_3, \lambda_1/\lambda_4, \dots$), soit de type 2 ($\lambda_1/\lambda_2, \lambda_2/\lambda_3, \lambda_3/\lambda_4, \dots$).

Etant donné un descripteur (de taille fixée) et une distance, nous avons séparé les 1050 images en deux catégories : un train set (composé d'environ 80\% des images) et un test set (constitué des images restantes). Pour chaque image du test set nous avons recherché l'image du train set qui minimise la distance utilisée. L'algorithme de classification réussit si les deux images appartiennent à la même catégorie.

Nous avons répété 20 fois l'expérience précédente (parallélisée sur 4 coeurs) afin d'obtenir des écarts types inférieurs à 0.03. Les pourcentages de réussites sont regroupés figure \ref{d1} pour le descripteur 1 et figure \ref{d2} pour le descripteur 2. La figure \ref{d3} illustre les performances lorsque le descripteur est composé en partie du type 1 et du type 2 (par exemple l'abscisse 30/10 correspond à un descripteur de taille 40 dont les 30 premières composantes sont de type 1 et les 10 dernières de type 2). Nous avons utilisé uniquement la métrique cosine pour ce dernier test.

\begin{figure}[H]
  \includegraphics[scale=1.15]{../plot/courbe1.pdf}
\caption{Performances du descripteur 1 selon la distance utilisée}
\label{d1}
\end{figure}

\begin{figure}[H]
  \includegraphics[scale=1.15]{../plot/courbe2.pdf}
\caption{Performances du descripteur 2 selon la distance utilisée}
\label{d2}
\end{figure}

\begin{figure}[H]
  \includegraphics[scale=1.15]{../plot/courbe3.pdf}
\caption{Performances des descripteurs  et 2 mélangés, selon la distance utilisée}
\label{d3}
\end{figure}

%#############################################################################################################%
%#############################################################################################################%
%#############################################################################################################%

\section{Discussion}

Le descripteur de type 1 ($\lambda_1/\lambda_2, \lambda_1/\lambda_3, \lambda_1/\lambda_4, \dots$) se révèle plus efficace. En effet, comme on peut le constater figure \ref{d1}, quelle que soit la métrique utilisée il permet de dépasser les 60\% de réussite, alors que le descripteur de type 2 (figure \ref{d2}) ne franchit pas 56\%. Mélangés les deux descripteurs ne permet pas non plus d'augmenter les performances comme l'illustre la figure \ref{d3}.

Le meilleur score est obtenu avec la métrique cosine pour un descripteur de type 1 de taille 30 (environ 70\% de réussite). Nous utilisons ces caractéristiques dans notre algorithme final.

Par ailleurs, la faible sensibilité aux perturbations (rotation, dilatation, bruit...) confirme notre hypothèse de départ : lorsque le domaine de l'image est faiblement changé, la propagation des ondes est peu impactée.

Afin d'améliorer la classification, plusieurs paramètres de l'algorithmes pourraient être étudiés plus en détail. Ainsi, on constate que le choix de la métrique est important (pour un descripteur fixé, les performances peuvent changer de 20\% entre deux métriques différentes). Une approche par machine learning pourrait permettre par exemple de pondérer chaque composante du descripteur (peut-être faut-il donner d'avantage d'importance à $\lambda_1/\lambda_2$ qu'à $\lambda_1/\lambda_3$).


%#############################################################################################################%
%#############################################################################################################%
%#############################################################################################################%

\section{Conclusion}

% images avec même ondes

%#############################################################################################################%
%#############################################################################################################%
%#############################################################################################################%

\section*{Bonus}

Nous avons essayé de reconstruire un son à partir des valeurs propres du Laplacien de Dirichlet. Pour cela, à partir du spectre des valeurs propres $\{\lambda_1, \lambda_2, ...\}$ de l'image, on en déduit les fréquences pouvant se propager dans la cavité définie par l'image, de la forme $\{\alpha \sqrt{\lambda_1}, \alpha \sqrt{\lambda_2}, ...\}$, $\alpha$ étant une constante choisie de façon à ce que le spectre obtenu soit compris entre 100Hz et 2kHz.

En pratique, $\alpha = 40$, on ne retient que 3 valeurs propres. Enfin on associe la même puissance à chacune des 3 fréquences calculées.

On peut s'amuser à changer ces différents paramètres pour entendre des sons différents.

Afin d'entendre le son associé à l'image \texttt{beetle-11.pgm} par exemple, entrer : 
\begin{center}
  \texttt{python3 sound.py database/beetle-11.pgm}
\end{center}

Nous avons également développé un petit jeu, accessible par :
\begin{center}
  \texttt{python3 sound\_game.py}
\end{center}


Il s'agit de retrouver parmi les sons de plusieurs objets celui appartenant à la même catégorie qu'un motif de départ. 

%#############################################################################################################%
%#############################################################################################################%
%#############################################################################################################%

\bibliographystyle{alpha}
\bibliography{Biblio}
\nocite{*}

\end{document}